{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 線形代数が使われる分野\n",
    "\n",
    "これまで，解ける連立方程式の理論と具体的な解法について学習してきました．\n",
    "教科書などで扱われてきた連立方程式の問題は，解き方を理解するための問題と言っていいでしょう．\n",
    "現実には，そのような問題に遭遇することは少ないでしょう．\n",
    "\n",
    "それでは，現実問題ではどんな連立方程式に遭遇するのでしょうか．\n",
    "それは，最先端のサイエンスにおいてふんだんに使われています．\n",
    "そして，それらにおいては巨大な係数行列を取扱う必要があります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その例を取り上げます．\n",
    "\n",
    "- ディープラーニングのニューラルネットワーク\n",
    "- 微分方程式の数値解法\n",
    "\n",
    "この講義では，これらの内容には踏み込みませんが，巨大な行列が扱われている事実について紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 巨大行列の特徴\n",
    "\n",
    "現実の問題で現れる巨大行列には幾つかの特徴があります．\n",
    "例えば，\n",
    "\n",
    "- 疎な行列\n",
    "- 帯行列\n",
    "- パターン化された行列\n",
    "\n",
    "などです．\n",
    "疎な行列とは，行列の成分の殆どがゼロで構成されているものです．\n",
    "帯行列とは，対角線成分の近くの成分以外がゼロで構成されているものです．\n",
    "パターン化された行列とは，成分の値が特定の規則で計算されるものです．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの行列を扱うときは計算方法を工夫することによって，計算コストを大幅に削減できることです．\n",
    "\n",
    "例えば$n$次正方行列どうしの積を行うとき，通常の行列の場合は$n^3$回のオーダーで数値の積を行わなければなりません．\n",
    "しかし，$n$次対角行列どうしの積ならば，$n$回の積で済みます．\n",
    "この違いは，小さな行列では問題になることはありませんが，巨大行列では計算コストの大きな違いとなります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使われない公式\n",
    "\n",
    "連立方程式を解くことは，行列の概念では逆行列を求めれば理論的には完了します．\n",
    "逆行列の求める公式として，Cramerの公式があります．\n",
    "どんな行列であっても正則ならば，画一的な計算アルゴリズムによって逆行列を求める方法です．\n",
    "現実問題に対面しなければCramerの公式を知っていれば一意的に解ける連立方程式の問題は終わっています．\n",
    "しかし，Cramerの公式の計算コストは極めて高く，巨大行列には相応しくないアプローチ方法となります．\n",
    "\n",
    "Gaussの消去法は，一般の行列については適切な計算コストを提供します．\n",
    "しかし，巨大行列においては，更に個別に工夫した計算アルゴリズムを用います．\n",
    "例えば，ディープラーニング用のパッケージでは，そこで扱われる巨大行列に最適化された計算アルゴリズムが採用されています．\n",
    "このような最先端のパッケージでは，表面には現れない技術がパッケージの真価になっています．\n",
    "\n",
    "- 計算回数を最小にする技術\n",
    "- 計算誤差を最小にする技術\n",
    "- 計算が不安定になる状況を回避する技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## ニューラルネットワークの行列\n",
    "\n",
    "ここでニューラルネットワークの理論に深入りすることはありませんが，ニューラルネットワークの簡単な例を通して巨大行列の関係について調べてみましょう．\n",
    "下図の例を考えます．\n",
    "\n",
    "![ニューラルネットワーク例](./images/nuralnetwork.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークでは，円で表現している部分をノードと言い，ノードを繋いでいる矢線をエッジと言います．\n",
    "データが，ノードからエッジに沿って伝搬していきます．\n",
    "伝搬するときに重みがかかります．\n",
    "\n",
    "#### 上記の図の説明\n",
    "\n",
    "- ノード$x_1,x_2,x_3$を入力層と言います．この層に入力データが与えられます．\n",
    "- ノード$x_4$から$x_8$を隠れ層と言います．隠れ層は，1段目が$x_4,x_5,x_6$で，2層目が$x_7,x_8$の2階層になっています．\n",
    "- ノード$x_9$を出力層と言います．この層が結果となります．\n",
    "- ノード$x_i$からノード$x_j$への伝搬についての重みを$w_{ij}$とします．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ノード間の関係式\n",
    "\n",
    "この例では，活性化関数などに関しては無視してネットワーク構造だけで話を進めます．\n",
    "上記の図から，次の式が得られます．\n",
    "\n",
    "$$\\left\\{\\begin{array}{l}\n",
    "w_{14}x_1 + w_{24}x_2 + w_{34}x_3 = x_4  \\\\\n",
    "w_{15}x_1 + w_{25}x_2 + w_{35}x_3 = x_5  \\\\\n",
    "w_{16}x_1 + w_{26}x_2 + w_{36}x_3 = x_6  \\\\\n",
    "w_{47}x_4 + w_{57}x_5 + w_{67}x_6 = x_7  \\\\\n",
    "w_{48}x_4 + w_{58}x_5 + w_{68}x_6 = x_8  \\\\\n",
    "w_{79}x_7 + w_{89}x_8  = x_9  \\\\\n",
    "\\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この連立方程式の左辺の係数行列を取り出すと次のようになります．\n",
    "\n",
    "$$ \\left( \\begin{array}{c}\n",
    "w_{14} & w_{24} & w_{34} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "w_{15} & w_{25} & w_{35} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "w_{16} & w_{26} & w_{36} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & w_{47} & w_{57} & w_{67} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & w_{48} & w_{58} & w_{68} & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & w_{79} & w_{89} \\\\\n",
    "\\end{array}\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この行列は6行8列の行列となります．\n",
    "この行列の特徴ですが，対角線付近にゼロ以外の数値があり，その他の成分はゼロになっています．\n",
    "\n",
    "このニューラルネットワークのモデルが教師付き学習であるとすると，計算で求まった$x_9$の値と実際の値との誤差を最小にするように重み係数$w_{ij}$を調整することが目的となります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例ではノードの個数が9個ですが，一般的なディープラーニングですと，隠れ層が10層で各層のノードが10個という規模は普通にあります．\n",
    "すると，ノードの個数が100個となるので，係数行列は100行100列規模の行列となります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "## 微分方程式の行列\n",
    "\n",
    "微分方程式で表される現象について具体的に解を求めるとき，微分方程式の理論を知っていても全く歯が立たないことが普通です．\n",
    "微分方程式の数値解法には様々なアプローチがありますが，最終的には行列計算になる場合が多いです．\n",
    "ここでは，そのような例を一つご紹介します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熱方程式\n",
    "\n",
    "熱の分布を扱う問題で定常状態を表す微分方程式は楕円型偏微分方程式です．\n",
    "簡単な例として，1次元空間での熱の定常状態を次の式で表します．\n",
    "\n",
    "> $$\\displaystyle -\\frac{d^2u}{{dx}^2} = q(x) $$\n",
    "\n",
    "ここで変数$x$の取りうる範囲を$0 \\le x \\le 1$として，境界条件を$u(0)=u(1)=0$とします．\n",
    "関数$q(x)$は，熱源です．\n",
    "この条件の下で，この微分方程式を解けば，1次元空間上での熱分布が求まります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 差分方程式\n",
    "\n",
    "微分方程式の数値解法の一つが差分方程式に近似して解く方法があります．\n",
    "2回の微分は，次のように差分近似することができます\n",
    "\n",
    "$$\\displaystyle \\frac{d^2u}{{dx}^2}(x) \\approx \\frac{u(x+\\delta)-2u(x)+u(x-\\delta)}{{\\delta}^2}$$\n",
    "\n",
    "ここで$\\delta$は極めて小さな正の数とします．\n",
    "この差分近似を使うと熱の定常状態の式は次のようになります．\n",
    "\n",
    "> $$\\displaystyle -u(x-\\delta)+2u(x)-u(x+\\delta) = {\\delta}^2q(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで，$\\delta=0.1$とし，変数$x$を$0.1,0.2,\\cdots,0.9$として，差分方程式に代入します．\n",
    "$u(0)=u(1)=0$に注意すると，一連の式が得られます．\n",
    "\n",
    "$$ \\left\\{ \\begin{array}{ccc}\n",
    "2u_1-u_2 &=& q_1 \\\\\n",
    "-u_1+2u_2-u_3 &=& q_2 \\\\\n",
    "\\vdots \\\\\n",
    "-u_7+2u_8-u_9 &=& q_8 \\\\\n",
    "-u_8+2u_9 &=& q_9 \\\\\n",
    "\\end{array} \\right. $$\n",
    "\n",
    "ここで，$u_k=u(0.1k)$，$q_k=0.01q(0.1k)$と置いています．\n",
    "このように，微分方程式を差分近似することによって，線形代数の問題に帰着します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この連立方程式を行列で表現すると，次のようになります．\n",
    "\n",
    "$$ \\left( \\begin{array}{c}\n",
    " 2 & -1 &  0 &  0 &  0 &  0 &  0 &  0 &  0 \\\\\n",
    "-1 &  2 & -1 &  0 &  0 &  0 &  0 &  0 &  0 \\\\\n",
    " 0 & -1 &  2 & -1 &  0 &  0 &  0 &  0 &  0 \\\\\n",
    " 0 &  0 & -1 &  2 & -1 &  0 &  0 &  0 &  0 \\\\\n",
    " 0 &  0 &  0 & -1 &  2 & -1 &  0 &  0 &  0 \\\\\n",
    " 0 &  0 &  0 &  0 & -1 &  2 & -1 &  0 &  0 \\\\\n",
    " 0 &  0 &  0 &  0 &  0 & -1 &  2 & -1 &  0 \\\\\n",
    " 0 &  0 &  0 &  0 &  0 &  0 & -1 &  2 & -1 \\\\\n",
    " 0 &  0 &  0 &  0 &  0 &  0 &  0 & -1 &  2 \\\\\n",
    "\\end{array} \\right) \n",
    "\\left( \\begin{array}{c}\n",
    "u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\\\ u_5 \\\\ u_6 \\\\ u_7 \\\\ u_8 \\\\ u_9\n",
    "\\end{array} \\right)\n",
    "=\n",
    "\\left( \\begin{array}{c}\n",
    "q_1 \\\\ q_2 \\\\ q_3 \\\\ q_4 \\\\ q_5 \\\\ q_6 \\\\ q_7 \\\\ q_8 \\\\ q_9\n",
    "\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この係数行列は極めて規則的な形状をしている帯行列です．\n",
    "この方程式はGaussの消去法で解くことができますが，この規則性を利用して計算量を圧縮した計算プロセスにします．\n",
    "\n",
    "この方程式は，差分近似をするときに差分を$0.1$刻みにしましたが，この刻み幅が小さいほど正確な記述となります．\n",
    "よって，刻み幅を$0.01$にすると，もっと正確な近似となりますが，そのときの係数行列は100行100列の巨大帯行列となります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "ここでは，ニューラルネットワークと微分方程式における巨大行列との遭遇をみてきました．\n",
    "線形代数が現代サイエンスの現実的な解法の基礎になっていることがご理解いただけたでしょうか．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
